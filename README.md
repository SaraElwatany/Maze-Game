# Churn Prediction with MLflow Tracking

## ğŸ“‘ Table of Contents

- [About the Project](#about-the-project)
- [Project Structure](#project-structure)
- [Setup Instructions](#setup-instructions)
- [Experimentation with MLflow](#experimentation-with-mlflow)
- [Model Selection & Results](#model-selection--results)
- [Staging vs Production Justification](#staging-vs-production-justification)
  

---

## ğŸ“Œ About the Project

This project demonstrates a full MLOps workflow for a **churn prediction** task. It includes:
- Model experimentation with scikit-learn  
- MLflow logging (parameters, metrics, models, input/output schema)  
- Model versioning and lifecycle management  
- Environment and dependency management with `venv`  
- Clear separation of research code on the `research` branch  

**Dataset**: Synthetic bank customer data with features like age, balance, credit score, etc.

---

## ğŸ“ Project Structure
```bash
MLOps-Course-Labs/
â”œâ”€â”€ churn_prediction/   # Virtual environment (untracked)
â”œâ”€â”€ data/               # Contains CSV dataset
â”œâ”€â”€ mlruns/             # MLflow run logs
â”œâ”€â”€ mlartifacts/         # MLflow artifact store
â”œâ”€â”€ src/
  â””â”€â”€ preprocessing.py
  â””â”€â”€ model.py
â”‚ â””â”€â”€ main.py
â”œâ”€â”€ testing/
  â””â”€â”€ __init__.py
  â””â”€â”€ test.py
â”œâ”€â”€ apis/
  â””â”€â”€ apis.py
  â””â”€â”€ requirements.txt
  â””â”€â”€ model.pkl
  â””â”€â”€ transformer.pkl
  â””â”€â”€ DockerFile      # API Image
â”œâ”€â”€ prometheus/
  â””â”€â”€ prometheus.yml
â”œâ”€â”€ grafana/
  â””â”€â”€ provisioning/
    â””â”€â”€ dashboards/
      â””â”€â”€ dashboard.yml
â”œâ”€â”€ model.pkl 
â”œâ”€â”€ transformer.pkl       # Saved transformer for preprocessing
â”œâ”€â”€ plot_confusion_matrix.png       # Evaluation visualization
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```


---

## âš™ï¸ Setup Instructions

1. **Clone and switch to the research branch**

```bash
git clone https://github.com/SaraElwatany/MLOps-Course-Labs.git
cd MLOps-Course-Labs
git checkout research
```


2. **Create and activate virtual environment**

```bash
python -m venv churn_prediction
churn_prediction\Scripts\activate  # On Windows
```


3. **Install dependencies**

```bash
pip install -r requirements.txt
```


4. **Run training script**
   
python src/main.py




---

## ğŸ”¬Experimentation with MLflow

Multiple experiments were run and tracked:

- Model type and hyperparameters

- Accuracy, precision, recall, and F1-score

- Confusion matrix visualizations

- Input and output schema using mlflow.models.infer_signature


Tracked models:

- Logistic Regression

- Random Forest
  
- Gradient Boosting
  


---

## ğŸ“Š Model Selection & Results

| Model             | Accuracy | Precision | Recall | F1 Score |
|-------------------|----------|-----------|--------|----------|
| Random Forest     | 0.76     |    0.76   |  0.73  |  0.74    |
| Gradient Boosting | 0.77     |    0.79   |  0.73  |  0.76    |


After conducting a minimum of 25 runs, the following conclusions were drawn based on the most promising results:

- **Gradient Boosting** consistently achieved the highest accuracy, along with superior recall and F1-score, making it the strongest candidate for production deployment due to its robust predictive performance.

- **Random Forest**, while exhibiting slightly lower metrics, benefits from lower computational complexity, which makes it a compelling choice for staging environments or scenarios that prioritize speed and resource efficiency.



---

## ğŸš¦ Staging vs Production Justification

- âœ… Staging Model: Random Forest

Rationale: Fast training time, easy to interpret, slightly lower performance

Use case: Ideal for testing environments or quick iterations


- ğŸ Production Model: Gradient Boosting

Rationale: Best overall performance and robustness

Use case: Deployed for live predictions in a real-world scenario

Both models were registered and versioned using MLflow Model Registry with proper tagging and descriptions.


